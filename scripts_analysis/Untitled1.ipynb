{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "val1 = \"/Users/yanjungao/Desktop/LGI4temporalgrounding-master/data/ActivityNet/captions/annotations/val_1.json\"\n",
    "val2 = \"/Users/yanjungao/Desktop/LGI4temporalgrounding-master/data/ActivityNet/captions/annotations/val_2.json\"\n",
    "train = \"/Users/yanjungao/Desktop/LGI4temporalgrounding-master/data/ActivityNet/captions/annotations/train.json\"\n",
    "train = json.load(open(train,'r')) \n",
    "val1 = json.load(open(val1,'r'))\n",
    "val2 = json.load(open(val2,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anns = {}\n",
    "qid = 0 \n",
    "for anns in [val1, val2]:\n",
    "    for vid in anns.keys():\n",
    "        ann = anns[vid]\n",
    "        duration = ann[\"duration\"]\n",
    "        descrps = \" \".join(ann[\"sentences\"]) \n",
    "        for ts,q in zip(ann[\"timestamps\"], ann[\"sentences\"]):\n",
    "            new_anns[str(qid)] = {\n",
    "                \"descriptions\": descrps\n",
    "            }\n",
    "            qid += 1\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(v['descriptions'].split()) for k,v in new_anns.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.std(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_anns = {} \n",
    "qid = 0 \n",
    "for vid in train.keys():\n",
    "    ann = train[vid]\n",
    "    duration = ann[\"duration\"]\n",
    "    for ts,q in zip(ann[\"timestamps\"], ann[\"sentences\"]):\n",
    "        train_new_anns[str(qid)] = {\n",
    "            \"timestamps\": ts,\n",
    "            \"query\": q,\n",
    "            \"video_id\": vid\n",
    "        }\n",
    "        qid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benepar \n",
    "parser = benepar.Parser(\"benepar_en2\")\n",
    "def parse_line(line):\n",
    "    tree = parser.parse(line)\n",
    "    item = None\n",
    "    for tr in tree.subtrees():\n",
    "        if tr.label() == 'VP':\n",
    "            item = tr\n",
    "    return item \n",
    "\n",
    "# Process queries by constituency parser, extract main verb (verb not in subordinating conjunction)\n",
    "# Main objects are also extracted \n",
    "t = \"person turns off the light as they're leaving\"\n",
    "\n",
    "from nltk.tree import ParentedTree\n",
    "def is_sbar(tr, v_cnt):\n",
    "    ptr = tr.parent()\n",
    "    flag = False \n",
    "    while ptr:\n",
    "        if ptr.label() == 'SBAR' and v_cnt !=0:\n",
    "            flag = True \n",
    "            break \n",
    "        else:\n",
    "            ptr = ptr.parent() \n",
    "    return flag \n",
    "\n",
    "verbs_tags = ['VBZ', 'VBP', 'VB', 'VBD','VBG', 'VBN']\n",
    "nouns_tags = ['NNS', 'NN']\n",
    "def find_verb(item):\n",
    "    verb = None \n",
    "    for st in item.subtrees():\n",
    "        if st.label() in verbs_tags:\n",
    "            verb = st.leaves()\n",
    "            return verb, st  \n",
    "    return verb, st  \n",
    "\n",
    "def find_noun(item):\n",
    "    noun = None \n",
    "    for st in item.subtrees():\n",
    "        if st.label() in nouns_tags:\n",
    "            noun = st.leaves()\n",
    "            return noun\n",
    "    return None \n",
    "\n",
    "# Extract main verbs and nouns as translation phrase \n",
    "def process_line(t):\n",
    "    line = t.split(\"##\")[-1]\n",
    "    tree = parser.parse(line)\n",
    "    newtree = ParentedTree.convert(tree)\n",
    "    output= [] \n",
    "    verbs = [] \n",
    "    nouns = [] \n",
    "    v_cnt = 0 \n",
    "    out = \"\"\n",
    "    for tr in newtree.subtrees():\n",
    "        if tr.label() == 'VP':\n",
    "            #flag = is_sbar(tr, v_cnt)\n",
    "            flag = False \n",
    "            if not flag:\n",
    "                verb, st = find_verb(tr)\n",
    "                if verb not in verbs:\n",
    "                    verbs.append(verb)\n",
    "                    v_cnt +=1 \n",
    "                    if verb:\n",
    "                        out += \" \"+ \" \".join(verb)\n",
    "                noun = find_noun(tr)\n",
    "                if noun not in nouns: \n",
    "                    nouns.append(noun)\n",
    "                    #print(noun)\n",
    "                    if noun:\n",
    "                        out += \" \"+ \" \".join(noun)\n",
    "    return verbs, nouns, out   \n",
    "\n",
    "stem_queries_verb = {}\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def process_pip(line):\n",
    "    verbs, nouns, out  = process_line(line)\n",
    "    words = [WordNetLemmatizer().lemmatize(w,'v') for w in out.split()]\n",
    "    return words \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_anns = {}\n",
    "for k,v in new_anns.items():\n",
    "    parsed_anns[k] = process_pip(v['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_trains= {}\n",
    "for k,v in train_new_anns.items():\n",
    "    parsed_trains[k] = process_pip(v['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs=[]\n",
    "for k,v in parsed_anns.items():\n",
    "    vocabs.extend(v)\n",
    "for k,v in parsed_trains.items():\n",
    "    vocabs.extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_idx = {k:list(set(vocabs)).index(k)+1 for k in vocabs}\n",
    "vocab_idx['PAD'] = 0 \n",
    "vocab_idx['<sos>'] = len(vocab_idx) \n",
    "vocab_idx['<eos>'] = len(vocab_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_idx)\n",
    "idx_vocab = {v:k for k,v in vocab_idx.items()}\n",
    "idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_index(queries, vocab_idx):\n",
    "    out_idx = {}\n",
    "    for k,v in queries.items():\n",
    "        v.insert(0, '<sos>')\n",
    "        v.append('<eos>')\n",
    "        val = [vocab_idx[i] for i in v]\n",
    "        out_idx[k] = val \n",
    "    return out_idx \n",
    "\n",
    "label_anns = label_index(parsed_anns, vocab_idx)\n",
    "train_anns = label_index(parsed_trains, vocab_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('anet_test_translate.json', 'w') as f:\n",
    "    json.dump([label_anns, parsed_anns], f)\n",
    "    \n",
    "with open('anet_train_translate.json', 'w') as f:\n",
    "    json.dump([train_anns, parsed_trains], f)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anet_vocab_translate.json','w') as f:\n",
    "    json.dump([vocab_idx, idx_vocab],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2['v_3iLo6lxAarc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anns['9405']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'A woman is seen speaking to the camera while holding up a box.'\n",
    "for k,v in val_text.items():\n",
    "    if v == t:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2['v_1epGZvRN3Fw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(list(val2.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['v_3iLo6lxAarc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(zip(ann[\"timestamps\"], ann[\"sentences\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
